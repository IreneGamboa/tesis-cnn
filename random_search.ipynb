{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from keras.layers import Input, Dense, Conv1D, Flatten, MaxPooling1D, Dropout\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "def network(input_window_size=60, filter_number=32, conv_window=(3,), pooling_window=(2,), dropout_rate=[],\n",
    "            activation='relu', dense_activation='softmax', optimizer='adam', loss='categorical_crossentropy', layers=1):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Input Layer\n",
    "    model.add(Conv1D(filter_number, conv_window, activation=activation, padding='same', input_shape=(input_window_size, 1)))\n",
    "    model.add(MaxPooling1D(pooling_window, padding='same'))\n",
    "    model.add(Dropout(dropout_rate[0]))\n",
    "\n",
    "    \n",
    "    # Hidden Layers\n",
    "    current_filter = 1\n",
    "    filter_number_temp = filter_number\n",
    "    for i in range(layers):\n",
    "        filter_number_temp = filter_number_temp *  2\n",
    "        model.add(Conv1D(filter_number_temp, conv_window, activation=activation, padding='same'))\n",
    "        model.add(MaxPooling1D(pooling_window, padding='same'))\n",
    "        model.add(Dropout(dropout_rate[current_filter]))\n",
    "        current_filter = current_filter + 1\n",
    "    \n",
    "    # Output Layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(2, activation=dense_activation))\n",
    "\n",
    "    model.compile(optimizer=optimizer[0], loss=loss, metrics=['accuracy', 'categorical_accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def rolling_window(a, window, step_size, padding=True, copy=True):\n",
    "    if copy:\n",
    "        result = a.copy()\n",
    "    else:\n",
    "        result = a\n",
    "    if padding:\n",
    "        result = np.hstack((result, np.zeros(window)))\n",
    "    shape = result.shape[:-1] + (result.shape[-1] - window + 1 - step_size, window)\n",
    "    strides = result.strides + (result.strides[-1] * step_size,)\n",
    "    return np.lib.stride_tricks.as_strided(result, shape=shape, strides=strides)\n",
    "\n",
    "\n",
    "size = 60\n",
    "\n",
    "base_path = '/home/irene/Documents/Datos-Tesis/Training_window_set_1'\n",
    "\n",
    "total_files = len(os.listdir(base_path))\n",
    "x_dataset = np.zeros(shape=(0, size))\n",
    "y_dataset = np.zeros(shape=(0, 1))\n",
    "count = 1\n",
    "skipped = []\n",
    "for file_path in os.listdir(base_path):\n",
    "    current_index = 0\n",
    "    \n",
    "    h5f = h5py.File(os.path.join(base_path, file_path), 'r')\n",
    "    \n",
    "    x = rolling_window(h5f['normal'][:], size, 1)\n",
    "    y = rolling_window(h5f['transformed'][:], size, 1)\n",
    "    y = [[np.amax(array)] for array in y]\n",
    "\n",
    "    x_dataset = np.vstack((x_dataset, x))\n",
    "    y_dataset = np.vstack((y_dataset, y))\n",
    "    \n",
    "    print(count)\n",
    "    count = count + 1\n",
    "\n",
    "x_dataset = np.reshape(x_dataset, (len(x_dataset), size, 1))\n",
    "y_dataset = np.reshape(y_dataset, (len(y_dataset), 1))\n",
    "y_dataset = to_categorical(y_dataset)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_dataset, y_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/irene/virtualenvs/obspy/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/irene/virtualenvs/obspy/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed: 196.9min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed: 196.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/irene/virtualenvs/obspy/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/30\n",
      "221471/221471 [==============================] - 86s 388us/step - loss: 0.1552 - acc: 0.9551 - categorical_accuracy: 0.9551\n",
      "Epoch 2/30\n",
      "221471/221471 [==============================] - 86s 387us/step - loss: 0.1276 - acc: 0.9640 - categorical_accuracy: 0.9640\n",
      "Epoch 3/30\n",
      "221471/221471 [==============================] - 85s 385us/step - loss: 0.1181 - acc: 0.9668 - categorical_accuracy: 0.9668\n",
      "Epoch 4/30\n",
      "221471/221471 [==============================] - 89s 402us/step - loss: 0.1133 - acc: 0.9676 - categorical_accuracy: 0.9676\n",
      "Epoch 5/30\n",
      "221471/221471 [==============================] - 89s 402us/step - loss: 0.1106 - acc: 0.9683 - categorical_accuracy: 0.9683\n",
      "Epoch 6/30\n",
      "221471/221471 [==============================] - 89s 402us/step - loss: 0.1086 - acc: 0.9690 - categorical_accuracy: 0.9690\n",
      "Epoch 7/30\n",
      "221471/221471 [==============================] - 89s 403us/step - loss: 0.1063 - acc: 0.9696 - categorical_accuracy: 0.9696\n",
      "Epoch 8/30\n",
      "221471/221471 [==============================] - 92s 418us/step - loss: 0.1044 - acc: 0.9700 - categorical_accuracy: 0.9700\n",
      "Epoch 9/30\n",
      "221471/221471 [==============================] - 92s 413us/step - loss: 0.1030 - acc: 0.9700 - categorical_accuracy: 0.9700\n",
      "Epoch 10/30\n",
      "221471/221471 [==============================] - 92s 415us/step - loss: 0.1020 - acc: 0.9706 - categorical_accuracy: 0.9706\n",
      "Epoch 11/30\n",
      "221471/221471 [==============================] - 92s 416us/step - loss: 0.1000 - acc: 0.9709 - categorical_accuracy: 0.9709\n",
      "Epoch 12/30\n",
      "221471/221471 [==============================] - 93s 418us/step - loss: 0.0988 - acc: 0.9714 - categorical_accuracy: 0.9714\n",
      "Epoch 13/30\n",
      "221471/221471 [==============================] - 92s 415us/step - loss: 0.0976 - acc: 0.9715 - categorical_accuracy: 0.9715\n",
      "Epoch 14/30\n",
      "221471/221471 [==============================] - 92s 416us/step - loss: 0.0966 - acc: 0.9716 - categorical_accuracy: 0.9716\n",
      "Epoch 15/30\n",
      "221471/221471 [==============================] - 92s 418us/step - loss: 0.0956 - acc: 0.9716 - categorical_accuracy: 0.9716\n",
      "Epoch 16/30\n",
      "221471/221471 [==============================] - 92s 418us/step - loss: 0.0953 - acc: 0.9717 - categorical_accuracy: 0.9717\n",
      "Epoch 17/30\n",
      "221471/221471 [==============================] - 93s 419us/step - loss: 0.0943 - acc: 0.9722 - categorical_accuracy: 0.9722\n",
      "Epoch 18/30\n",
      "221471/221471 [==============================] - 93s 420us/step - loss: 0.0930 - acc: 0.9723 - categorical_accuracy: 0.9723\n",
      "Epoch 19/30\n",
      "221471/221471 [==============================] - 94s 423us/step - loss: 0.0926 - acc: 0.9724 - categorical_accuracy: 0.9724\n",
      "Epoch 20/30\n",
      "221471/221471 [==============================] - 93s 420us/step - loss: 0.0921 - acc: 0.9724 - categorical_accuracy: 0.9724\n",
      "Epoch 21/30\n",
      "221471/221471 [==============================] - 93s 422us/step - loss: 0.0909 - acc: 0.9726 - categorical_accuracy: 0.9726\n",
      "Epoch 22/30\n",
      "221471/221471 [==============================] - 94s 424us/step - loss: 0.0901 - acc: 0.9729 - categorical_accuracy: 0.9729\n",
      "Epoch 23/30\n",
      "221471/221471 [==============================] - 94s 423us/step - loss: 0.0899 - acc: 0.9729 - categorical_accuracy: 0.9729\n",
      "Epoch 24/30\n",
      "221471/221471 [==============================] - 94s 425us/step - loss: 0.0887 - acc: 0.9735 - categorical_accuracy: 0.9735\n",
      "Epoch 25/30\n",
      "221471/221471 [==============================] - 95s 428us/step - loss: 0.0889 - acc: 0.9729 - categorical_accuracy: 0.9729\n",
      "Epoch 26/30\n",
      "221471/221471 [==============================] - 94s 427us/step - loss: 0.0881 - acc: 0.9733 - categorical_accuracy: 0.9733\n",
      "Epoch 27/30\n",
      "221471/221471 [==============================] - 95s 427us/step - loss: 0.0875 - acc: 0.9735 - categorical_accuracy: 0.9735\n",
      "Epoch 28/30\n",
      "221471/221471 [==============================] - 95s 428us/step - loss: 0.0862 - acc: 0.9736 - categorical_accuracy: 0.9736\n",
      "Epoch 29/30\n",
      "221471/221471 [==============================] - 92s 414us/step - loss: 0.0858 - acc: 0.9737 - categorical_accuracy: 0.9737\n",
      "Epoch 30/30\n",
      "221471/221471 [==============================] - 92s 413us/step - loss: 0.0857 - acc: 0.9738 - categorical_accuracy: 0.9738\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint, uniform\n",
    "from keras import optimizers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Convert to scikit-learn model (build_fn is a function that returns a model)\n",
    "model = KerasClassifier(build_fn=network)\n",
    "\n",
    "# list of parameters\n",
    "input_window_size = [60]\n",
    "layers = [1]\n",
    "filter_number = [32, 64]\n",
    "conv_window = [(5,)]\n",
    "pooling_window = [(2,)]\n",
    "activation = ['relu']\n",
    "dense_activation = ['softmax', 'sigmoid']\n",
    "optimizer = [('adam', 0)]\n",
    "loss = ['binary_crossentropy']\n",
    "dropout_rate = [[0.1, 0.2, 0.1, 0.1]]\n",
    "\n",
    "epochs= [30]\n",
    "batch_size = [32]\n",
    "\n",
    "param_dist = dict(input_window_size=input_window_size,\n",
    "                  filter_number=filter_number, \n",
    "                  conv_window=conv_window, \n",
    "                  pooling_window=pooling_window,\n",
    "                  dropout_rate = dropout_rate,\n",
    "                  activation=activation, \n",
    "                  dense_activation=dense_activation, \n",
    "                  optimizer=optimizer, \n",
    "                  loss=loss, \n",
    "                  layers=layers,\n",
    "                  epochs=epochs,\n",
    "                  batch_size=batch_size)\n",
    "\n",
    "# Random Search\n",
    "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, cv=3, verbose=2, n_iter=4, n_jobs=-1)\n",
    "result = random_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.974385 using {'pooling_window': (2,), 'optimizer': ('adam', 0), 'loss': 'binary_crossentropy', 'layers': 1, 'input_window_size': 60, 'filter_number': 64, 'epochs': 30, 'dropout_rate': [0.1, 0.2, 0.1, 0.1], 'dense_activation': 'softmax', 'conv_window': (5,), 'batch_size': 32, 'activation': 'relu'}\n",
      "0.972746 (0.001016) with: {'pooling_window': (2,), 'optimizer': ('adam', 0), 'loss': 'binary_crossentropy', 'layers': 1, 'input_window_size': 60, 'filter_number': 32, 'epochs': 30, 'dropout_rate': [0.1, 0.2, 0.1, 0.1], 'dense_activation': 'softmax', 'conv_window': (5,), 'batch_size': 32, 'activation': 'relu'}\n",
      "0.974385 (0.000980) with: {'pooling_window': (2,), 'optimizer': ('adam', 0), 'loss': 'binary_crossentropy', 'layers': 1, 'input_window_size': 60, 'filter_number': 64, 'epochs': 30, 'dropout_rate': [0.1, 0.2, 0.1, 0.1], 'dense_activation': 'softmax', 'conv_window': (5,), 'batch_size': 32, 'activation': 'relu'}\n",
      "0.972568 (0.000964) with: {'pooling_window': (2,), 'optimizer': ('adam', 0), 'loss': 'binary_crossentropy', 'layers': 1, 'input_window_size': 60, 'filter_number': 32, 'epochs': 30, 'dropout_rate': [0.1, 0.2, 0.1, 0.1], 'dense_activation': 'sigmoid', 'conv_window': (5,), 'batch_size': 32, 'activation': 'relu'}\n",
      "0.974030 (0.000632) with: {'pooling_window': (2,), 'optimizer': ('adam', 0), 'loss': 'binary_crossentropy', 'layers': 1, 'input_window_size': 60, 'filter_number': 64, 'epochs': 30, 'dropout_rate': [0.1, 0.2, 0.1, 0.1], 'dense_activation': 'sigmoid', 'conv_window': (5,), 'batch_size': 32, 'activation': 'relu'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best: %f using %s\" % (result.best_score_, result.best_params_))\n",
    "means = result.cv_results_['mean_test_score']\n",
    "stds = result.cv_results_['std_test_score']\n",
    "params = result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "  print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = network(activation='relu', conv_window=(3,), dense_activation='sigmoid', dropout_rate=[0.15344147, 0.10309187, 0.05869609, 0.61434385, 0.66912969], filter_number=20, input_window_size=60, loss='categorical_crossentropy', optimizer=['adam'], pooling_window=(2,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.fit(x_train, y_train, epochs=30, batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "obspy",
   "language": "python",
   "name": "obspy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
