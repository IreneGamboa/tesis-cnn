{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from keras.layers import Input, Dense, Conv1D, Flatten, MaxPooling1D, Dropout\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l1_l2\n",
    "import h5py\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer, StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "import uuid\n",
    "import random\n",
    "import json\n",
    "  \n",
    "\n",
    "def network(input_window_size=60, filter_number=32, conv_window=(3,), pooling_window=(2,), dropout_rate=[],\n",
    "            activation='relu', dense_activation='softmax', optimizer='adam', loss='categorical_crossentropy', layers=1,\n",
    "            l1_value=0.0001, l2_value=0.0001):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Input Layer\n",
    "    model.add(Conv1D(filter_number, conv_window, activation=activation, padding='same', input_shape=(input_window_size, 1),\n",
    "                     activity_regularizer=l1_l2(l1=l1_value, l2=l2_value)))\n",
    "    model.add(MaxPooling1D(pooling_window, padding='same'))\n",
    "    model.add(Dropout(dropout_rate[0]))\n",
    "\n",
    "    \n",
    "    # Hidden Layers\n",
    "    current_filter = 1\n",
    "    filter_number_temp = filter_number\n",
    "    for i in range(layers):\n",
    "        filter_number_temp = filter_number_temp *  2\n",
    "        model.add(Conv1D(filter_number_temp, conv_window, activation=activation, padding='same', \n",
    "                         activity_regularizer=l1_l2(l1=l1_value, l2=l2_value)))\n",
    "        model.add(MaxPooling1D(pooling_window, padding='same'))\n",
    "        model.add(Dropout(dropout_rate[current_filter]))\n",
    "        current_filter = current_filter + 1\n",
    "    \n",
    "    # Output Layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(2, activation=dense_activation))\n",
    "\n",
    "    model.compile(optimizer=optimizer[0], loss=loss, metrics=['accuracy', 'categorical_accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def rolling_window(a, window, step_size, padding=True, copy=True):\n",
    "    if copy:\n",
    "        result = a.copy()\n",
    "    else:\n",
    "        result = a\n",
    "    if padding:\n",
    "        result = np.hstack((result, np.zeros(window)))\n",
    "    shape = result.shape[:-1] + (result.shape[-1] - window + 1 - step_size, window)\n",
    "    strides = result.strides + (result.strides[-1] * step_size,)\n",
    "    return np.lib.stride_tricks.as_strided(result, shape=shape, strides=strides)\n",
    "\n",
    "\n",
    "def load_data(size, base_path, output_path, scaler, normalizer=False):\n",
    "    output_path = output_path.format(size, scaler, normalizer)\n",
    "    total_files = len(os.listdir(base_path))\n",
    "    x_dataset = np.zeros(shape=(0, size))\n",
    "    y_dataset = np.zeros(shape=(0, 1))\n",
    "    for file_path in os.listdir(base_path):\n",
    "        # Cargar datos\n",
    "        h5f = h5py.File(os.path.join(base_path, file_path), 'r')\n",
    "        x = h5f['normal'][:]\n",
    "        y = rolling_window(h5f['transformed'][:], size, 1)\n",
    "\n",
    "        #Escalar datos antes de hacer el window\n",
    "        if scaler != 'none': \n",
    "            if scaler == 'min_max_scaler':\n",
    "                scaler = MinMaxScaler()\n",
    "            elif scaler == 'standard_scaler':\n",
    "                scaler = StandardScaler()\n",
    "            elif scaler == 'min_max_scaler_1':\n",
    "                scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "            scaler = scaler.fit(x.reshape(-1, 1))\n",
    "            x = scaler.transform(x.reshape(1, -1))[0]\n",
    "        if normalizer:\n",
    "            normalizer = Normalizer().fit(x.reshape(-1, 1))\n",
    "            x = normalizer.transform(x.reshape(1, -1))[0]\n",
    "\n",
    "        x = rolling_window(x, size, 1)\n",
    "\n",
    "        # Determina si P esta en el window o no\n",
    "        y = [[np.amax(array)] for array in y]\n",
    "\n",
    "        x_dataset = np.vstack((x_dataset, x))\n",
    "        y_dataset = np.vstack((y_dataset, y))\n",
    "\n",
    "    x_dataset = np.reshape(x_dataset, (len(x_dataset), size, 1))\n",
    "    y_dataset = np.reshape(y_dataset, (len(y_dataset), 1))\n",
    "\n",
    "    # Balancear datos:\n",
    "    # n = tama√±o del que haya menos labels \n",
    "    n = min(len(np.where(y_dataset == 1)[0]), len(np.where(y_dataset == 0)[0]))\n",
    "\n",
    "    # Elije n muestras aleatorias de cada label\n",
    "    mask = np.hstack([np.random.choice(np.where(y_dataset == l)[0], n, replace=False)\n",
    "                        for l in np.unique(y_dataset)])\n",
    "    x_dataset = x_dataset[mask]\n",
    "    y_dataset = y_dataset[mask]\n",
    "\n",
    "    # crea un one-hot-encoding vector de y_dataset\n",
    "    y_dataset = to_categorical(y_dataset)\n",
    "\n",
    "    dataset = h5py.File(output_path)\n",
    "    dataset.create_dataset('x_dataset', data=x_dataset)\n",
    "    dataset.create_dataset('y_dataset', data=y_dataset)\n",
    "    dataset.close()\n",
    "\n",
    "    print(\"Done creating data\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done creating data\n",
      "Done creating data\n",
      "Done creating data\n",
      "Done creating data\n"
     ]
    }
   ],
   "source": [
    "data_path = '/home/irene/Documents/Datos-Tesis/Training_window_set_1'\n",
    "output_path = '/home/irene/Documents/Datos-Tesis/window_output/{}_{}_{}.h5'\n",
    "\n",
    "for size in [60, 120]:\n",
    "    for scaler in ['none']:\n",
    "        for normalizer in [True, False]:\n",
    "            load_data(size, data_path, output_path, scaler, normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "obspy",
   "language": "python",
   "name": "obspy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
